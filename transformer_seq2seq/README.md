# PyTorch Transformer Seq2Seq Dialogue Generation

This is a Transformer-based sequence-to-sequence (seq2seq) dialogue generation project. It builds a complete Transformer model and trains it on the Cornell Movie Dialogs dataset to generate conversational responses.

This project uses a modular design, **aimed at mastering the practical nuts-and-bolts of Transformers and more advanced PyTorch usage.**.

### ðŸš€ 1. Environment Setup

This project was developed with **Python 3.12.12**. Please follow the steps below to set up the environment.

```bash
# 1. Make sure pip is up to date
pip install --upgrade pip

# 2. Install all dependencies
pip install --no-cache-dir -r requirements.txt
```

### ðŸ“‚ 2. Project Structure

```text
transformer_seq2seq/
â”œâ”€â”€ attention.py        # Multi-head attention mechanism
â”œâ”€â”€ config.py           # Configuration center
â”œâ”€â”€ dataset.py          # Dataset loading and processing
â”œâ”€â”€ decoder.py          # Decoder layer and stack
â”œâ”€â”€ embeddings.py       # Token embedding + positional encoding
â”œâ”€â”€ encoder.py          # Encoder layer and stack
â”œâ”€â”€ feedforward.py      # Position-wise feedforward network
â”œâ”€â”€ inference_demo.py   # Interactive inference demo
â”œâ”€â”€ mask.py             # Mask generation tools
â”œâ”€â”€ requirements.txt    # Dependency list
â”œâ”€â”€ train_demo.py       # Training demo
â””â”€â”€ transformer.py      # Complete Transformer model
```

### âš¡ 3. Model Training

Running the training script will automatically download the Cornell Movie Dialogs dataset, process the vocabulary, and start the training:

```bash
python train_demo.py
```

During training:
- The model uses teacher forcing for efficient training
- Gradient clipping is applied to prevent gradient explosion
- A warmup learning rate scheduler is used
- The trained model, vocabulary, and configuration will be saved to `transformer_cornell_full.pth`

### ðŸ’¬ 4. Dialogue Generation

After training, you can use the interactive inference demo to test the dialogue generation:

```bash
python inference_demo.py
```

Enter your message and the bot will generate a response based on the trained model.

### ðŸ“‹ 5. Core Implementation Details

#### Transformer Architecture  
The model implements the full Transformer architecture:  
- **Embedding Layer**: combines word embeddings with sinusoidal positional encodings  
- **Multi-Head Attention**: parallel attention heads capturing different contextual aspects  
- **Encoder**: stacked self-attention layers + feed-forward networks  
- **Decoder**: stacked masked self-attention + cross-attention + feed-forward networks  
- **Output Layer**: projects decoder outputs to vocabulary size  

```
transformer.py |<- embeddings.py
               |<- encoder.py  <- attention.py
               |<- decoder.py  <- attention.py
               |<- mask.py
```

#### Training Pipeline  
1. **Data Preparation**: auto-download & preprocess Cornell Movie Dialogs, build vocabulary, create DataLoader  
2. **Model Setup**: config_init, model_init, criterion, optimizer, ...  
3. **Training Loop**: full training and evaluation routine  
4. **Model Checkpointing**: save the complete model, vocabulary, and configuration  

```
train_demo.py |<- dataset.py
              |<- config.py
              |<- transformer.py
```

#### Inference Pipeline  
After training, the chat-generation flow is:  
1. **Model Loading**: load the trained model, vocabulary, and config from `transformer_cornell_full.pth`  
2. **User Input, Tokenization, Numericalization**: convert user input â†’ tokens â†’ token IDs  
3. **greedy_decode**  
4. **Text Conversion**: map generated token IDs back to text and print

# åŸºäºŽPytorchçš„Transformer Seq2Seqå¯¹è¯ç”Ÿæˆ

è¿™æ˜¯ä¸€ä¸ªåŸºäºŽTransformerçš„åºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰å¯¹è¯ç”Ÿæˆé¡¹ç›®ã€‚å®ƒæž„å»ºäº†å®Œæ•´çš„Transformeræ¨¡åž‹ï¼Œå¹¶åœ¨Cornellç”µå½±å¯¹è¯æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥ç”Ÿæˆå¯¹è¯å“åº”ã€‚

è¯¥é¡¹ç›®é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œ**æ—¨åœ¨å­¦ä¹ Transformerçš„å®žè·µç»†èŠ‚å’ŒPytorchçš„æ›´å¤šç”¨æ³•**ã€‚

### ðŸš€ 1. çŽ¯å¢ƒé…ç½®

å®žéªŒåŸºäºŽ **Python 3.12.12** å¼€å‘ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤é…ç½®çŽ¯å¢ƒã€‚

```bash
# 1. ç¡®ä¿ pip æ˜¯æœ€æ–°çš„
pip install --upgrade pip

# 2. å®‰è£…æ‰€æœ‰ä¾èµ–
pip install --no-cache-dir -r requirements.txt
```

### ðŸ“‚ 2. é¡¹ç›®ç»“æž„

```text
transformer_seq2seq/
â”œâ”€â”€ attention.py        # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
â”œâ”€â”€ config.py           # é…ç½®ä¸­å¿ƒ
â”œâ”€â”€ dataset.py          # æ•°æ®é›†åŠ è½½ä¸Žå¤„ç†
â”œâ”€â”€ decoder.py          # Decoderå±‚ä¸Žå †å 
â”œâ”€â”€ embeddings.py       # è¯åµŒå…¥ + ä½ç½®ç¼–ç 
â”œâ”€â”€ encoder.py          # Encoderå±‚ä¸Žå †å 
â”œâ”€â”€ feedforward.py      # ä½ç½®å‰é¦ˆç½‘ç»œ
â”œâ”€â”€ inference_demo.py   # äº¤äº’å¼æŽ¨ç†æ¼”ç¤º
â”œâ”€â”€ mask.py             # Maskç”Ÿæˆå·¥å…·
â”œâ”€â”€ requirements.txt    # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ train_demo.py       # è®­ç»ƒæ¼”ç¤º
â””â”€â”€ transformer.py      # å®Œæ•´Transformeræ¨¡åž‹
```

### âš¡ 3. æ¨¡åž‹è®­ç»ƒ

è¿è¡Œè®­ç»ƒè„šæœ¬å°†è‡ªåŠ¨ä¸‹è½½Cornellç”µå½±å¯¹è¯æ•°æ®é›†ã€å¤„ç†è¯è¡¨å¹¶å¼€å§‹è®­ç»ƒï¼š

```bash
python train_demo.py
```

è®­ç»ƒè¿‡ç¨‹ä¸­ï¼š
- æ¨¡åž‹ä½¿ç”¨teacher forcingè¿›è¡Œé«˜æ•ˆè®­ç»ƒ
- åº”ç”¨æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- ä½¿ç”¨warmupå­¦ä¹ çŽ‡è°ƒåº¦å™¨
- è®­ç»ƒå®ŒæˆåŽï¼Œæ¨¡åž‹ã€è¯è¡¨å’Œé…ç½®å°†ä¿å­˜åˆ° `transformer_cornell_full.pth`

### ðŸ’¬ 4. å¯¹è¯ç”Ÿæˆ

è®­ç»ƒå®ŒæˆåŽï¼Œä½ å¯ä»¥ä½¿ç”¨äº¤äº’å¼æŽ¨ç†æ¼”ç¤ºæµ‹è¯•å¯¹è¯ç”Ÿæˆï¼š

```bash
python inference_demo.py
```

è¾“å…¥ä½ çš„æ¶ˆæ¯ï¼Œæœºå™¨äººå°†æ ¹æ®è®­ç»ƒå¥½çš„æ¨¡åž‹ç”Ÿæˆå“åº”ã€‚

### ðŸ“‹ 5. æ ¸å¿ƒå®žçŽ°ç»†èŠ‚

#### Transformeræž¶æž„
æ¨¡åž‹å®žçŽ°äº†å®Œæ•´çš„Transformeræž¶æž„ï¼š
- **åµŒå…¥å±‚**ï¼šç»“åˆè¯åµŒå…¥ä¸Žæ­£å¼¦ä½ç½®ç¼–ç 
- **å¤šå¤´æ³¨æ„åŠ›**ï¼šå¹¶è¡Œæ³¨æ„åŠ›æœºåˆ¶æ•èŽ·ä¸Šä¸‹æ–‡çš„ä¸åŒæ–¹é¢
- **ç¼–ç å™¨**ï¼šè‡ªæ³¨æ„åŠ›å±‚å †å  + å‰é¦ˆç½‘ç»œ
- **è§£ç å™¨**ï¼šæŽ©ç è‡ªæ³¨æ„åŠ› + äº¤å‰æ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œå †å 
- **è¾“å‡ºå±‚**ï¼šå°†è§£ç å™¨è¾“å‡ºæŠ•å½±åˆ°è¯è¡¨å¤§å°

```
transformer.py |<-embeddings.py
			  |<-encoder.py <- attention.py
			  |<-decoder.py <- attention.py
			  |<-mask.py
```

#### è®­ç»ƒæµç¨‹
1. **æ•°æ®å‡†å¤‡**ï¼šè‡ªåŠ¨ä¸‹è½½ã€å¤„ç†CornellæˆåŽŸå§‹æ•°æ®å’Œæž„å»ºè¯è¡¨ã€æž„å»ºdataloader
2. **æ¨¡åž‹é…ç½®å’Œè®­ç»ƒå‡†å¤‡**ï¼šconfig_initã€model_initã€criterionã€optimizer...
4. **è®­ç»ƒå¾ªçŽ¯**ï¼šå®žçŽ°å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹
5. **æ¨¡åž‹ä¿å­˜**ï¼šä¿å­˜å®Œæ•´çš„æ¨¡åž‹ã€è¯è¡¨å’Œé…ç½®

```
train_demo.py | <- dataset.py
			 | <- config.py
			 | <- transfomer.py
```

#### æŽ¨ç†æµç¨‹
è®­ç»ƒå®ŒæˆåŽï¼Œç”Ÿæˆå¯¹è¯çš„æµç¨‹å¦‚ä¸‹ï¼š
1. **æ¨¡åž‹åŠ è½½**ï¼šä»Žä¿å­˜çš„æ£€æŸ¥ç‚¹ `transformer_cornell_full.pth` ä¸­åŠ è½½è®­ç»ƒå¥½çš„æ¨¡åž‹ã€è¯è¡¨å’Œé…ç½®
2. **ç”¨æˆ·è¾“å…¥ã€åˆ†è¯ã€æ•°å€¼åŒ–**ï¼šç”¨æˆ·è¾“å…¥è½¬æ¢ä¸ºtokens è½¬æ¢ä¸ºtoken IDs
3. **greedy_decode** 
4. **æ–‡æœ¬è½¬æ¢**ï¼šå°†ç”Ÿæˆçš„token IDsè½¬æ¢å›žæ–‡æœ¬å¹¶è¾“å‡º
