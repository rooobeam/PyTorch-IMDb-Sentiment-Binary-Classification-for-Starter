# ====================================
# æ–‡ä»¶: dataset.py
# æ•°æ®é›†åŠ è½½ï¼ˆä½¿ç”¨Cornellç”µå½±å¯¹è¯æ•°æ®é›†ï¼‰
# ====================================

import torch
from torch.utils.data import Dataset, DataLoader
import re
from collections import Counter
import urllib.request
import zipfile
from pathlib import Path
from typing import List

class Vocabulary:
    """è¯è¡¨æ„å»ºå·¥å…·"""

    def __init__(self, freq_threshold=2):
        # ç‰¹æ®Štoken
        self.pad_token = "<PAD>"
        self.bos_token = "<BOS>"
        self.eos_token = "<EOS>"
        self.unk_token = "<UNK>"

        self.freq_threshold = freq_threshold
        self.itos = {0: self.pad_token, 1: self.bos_token,
                     2: self.eos_token, 3: self.unk_token}
        self.stoi = {v: k for k, v in self.itos.items()}

    def build_vocabulary(self, sentence_list):
        """ä»å¥å­åˆ—è¡¨æ„å»ºè¯è¡¨"""
        counter = Counter()
        for sentence in sentence_list:
            counter.update(sentence)

        #  string to index
        idx = len(self.stoi)
        for word, freq in counter.items():
            if freq >= self.freq_threshold:
                self.stoi[word] = idx
                self.itos[idx] = word
                idx += 1

    def numericalize(self, text:List[str]):
        """æ–‡æœ¬è½¬token ids"""
        unk_idx = self.stoi[self.unk_token]
        return [self.stoi.get(token, unk_idx) for token in text]

    def __len__(self):
        return len(self.stoi)
class Vocabulary:
    def __init__(self,freq_threshold=2):
        self.pad_token = '<PAD>'
        self.bos_token = '<BOS>'
        self.eos_token = '<EOS>'
        self.unk_token = '<UNK>'
        self.freq_threshold=freq_threshold
        self.itos = {0: self.pad_token, 1: self.bos_token,
                     2: self.eos_token, 3: self.unk_token}
        self.stoi = {v: k for k, v in self.itos.items()}

    def build_vocab(self, all_sentences):
        counter = Counter()
        for sentence in all_sentences:
            counter.update(sentence)
        index = len(self.stoi)
        for word, freq in counter:
            if freq>=self.freq_threshold:
                self.itos[index] = word
                self.


    def numericalize(self, text):

    def __len__(self):
        return

class DialogueDataset(Dataset):
    """å¯¹è¯æ•°æ®é›†"""

    def __init__(self, data_pairs, vocab):
        self.data_pairs = data_pairs
        self.vocab = vocab

    def __len__(self):
        return len(self.data_pairs)

    def __getitem__(self, idx):
        src_text, tgt_text = self.data_pairs[idx]

        # è½¬ä¸ºidï¼ˆæ·»åŠ BOS/EOSï¼‰
        src_ids = [self.vocab.stoi["<BOS>"]] + \
                  self.vocab.numericalize(src_text) + \
                  [self.vocab.stoi["<EOS>"]]

        tgt_ids = [self.vocab.stoi["<BOS>"]] + \
                  self.vocab.numericalize(tgt_text) + \
                  [self.vocab.stoi["<EOS>"]]

        return torch.tensor(src_ids), torch.tensor(tgt_ids)


def simple_tokenize(text):
    """
    ç®€å•åˆ†è¯ï¼šå°å†™+ç©ºæ ¼åˆ†å‰²
    â€œæŠŠä¸åœ¨ç™½åå•é‡Œçš„å­—ç¬¦ç»Ÿç»Ÿåˆ æ‰â€ã€‚
    ç™½åå•é‡Œæœ‰å“ªäº›ï¼Ÿ aâ€“z 0â€“9 \sï¼ˆç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç­‰æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼‰ ?!.,â€˜
    """
    text = text.lower()
    text = re.sub(r"[^a-z0-9\s\?\!\.\,']", "", text)  # ä¿ç•™åŸºæœ¬æ ‡ç‚¹
    tokens = text.split()
    return tokens


def collate_fn(batch, pad_idx=0):
    """åŠ¨æ€Paddingåˆ°batchå†…æœ€å¤§é•¿åº¦"""
    src_batch, tgt_batch = zip(*batch)

    # è·å–batchå†…æœ€å¤§é•¿åº¦
    src_max_len = max(len(s) for s in src_batch)
    tgt_max_len = max(len(t) for t in tgt_batch)

    # Padding
    src_padded = torch.stack([
        torch.cat([s, torch.tensor([pad_idx] * (src_max_len - len(s)), dtype=torch.int32)])
        for s in src_batch
    ])

    tgt_padded = torch.stack([
        torch.cat([t, torch.tensor([pad_idx] * (tgt_max_len - len(t)), dtype=torch.int32)])
        for t in tgt_batch
    ])

    return src_padded, tgt_padded


def download_cornell_data(data_dir: Path = Path('./data')):
    """ä¸‹è½½Cornell Movie Dialogsæ•°æ®é›†"""
    data_dir.mkdir(parents = True, exist_ok = True)

    zip_file = data_dir / Path('cornell_movie_dialogs.zip') # æ–‡ä»¶ç”¨path
    extract_dir = data_dir / Path('cornell movie-dialogs corpus') # ç›®å½•ç”¨dir
    url = 'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip'

    if not zip_file.exists():
        print(f"ğŸ“¥ ä¸‹è½½Cornellæ•°æ®é›†...")
        urllib.request.urlretrieve(url, zip_file)
        print("âœ… ä¸‹è½½å®Œæˆ")

    # è§£å‹
    if not extract_dir.exists():
        print("ğŸ“¦ è§£å‹æ–‡ä»¶...")
        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
            zip_ref.extractall(data_dir)
        print("âœ… è§£å‹å®Œæˆ")

    return extract_dir


# def load_cornell_dialogues(data_dir: Path = Path('./data'), max_samples=5000, max_len=15):
#     """
#     åŠ è½½Cornellç”µå½±å¯¹è¯æ•°æ®é›†
#
#     å‚æ•°ï¼š
#     - max_samples: æœ€å¤šåŠ è½½å¤šå°‘å¯¹å¯¹è¯ï¼ˆé»˜è®¤5000ï¼Œé€‚åˆå¿«é€Ÿå®éªŒï¼‰
#     - max_len: è¿‡æ»¤æ‰è¶…è¿‡æ­¤é•¿åº¦çš„å¥å­ï¼ˆé¿å…è¿‡é•¿å¥å­ï¼‰
#     """
#     # ä¸‹è½½æ•°æ®
#     corpus_dir = download_cornell_data(data_dir)
#
#     # æ–‡ä»¶è·¯å¾„
#     lines_file = corpus_dir / Path('movie_lines.txt')
#     conv_file = corpus_dir / Path('movie_conversations.txt')
#     assert(lines_file.exists() and conv_file.exists())
#
#     # è§£æå°è¯ id2line
#     print("ğŸ“– è§£æç”µå½±å°è¯...")
#     id2line = {}
#     with open(lines_file, 'r', encoding='iso-8859-1') as f:
#         for line in f:
#             parts = line.strip().split(' +++$+++ ')
#             if len(parts) == 5:
#                 line_id = parts[0]
#                 text = parts[-1]
#                 id2line[line_id] = text
#
#     # æ„é€ å¯¹è¯ dialogues List[Tuple]
#     print("ğŸ”— æ„å»ºå¯¹è¯å¯¹...")
#     dialogues = []
#     with open(conv_file, 'r', encoding='iso-8859-1') as f:
#         for line in f:
#             parts = line.strip().split(' +++$+++ ')
#             if len(parts) == 4:
#                 line_ids = eval(parts[-1])  # ['L1', 'L2', 'L3']
#                 # æ„å»ºQ-Aå¯¹
#                 for i in range(len(line_ids) - 1):
#                     q_id = line_ids[i]
#                     a_id = line_ids[i + 1]
#                     if q_id in id2line and a_id in id2line:
#                         question = id2line[q_id]
#                         answer = id2line[a_id]
#                         dialogues.append((question, answer))
#
#     print(f"ğŸ“Š åŸå§‹å¯¹è¯æ•°é‡: {len(dialogues)}")
#
#     # åˆ†è¯ + è¿‡æ»¤é•¿å¥
#     print("âœ‚ï¸  åˆ†è¯å¹¶è¿‡æ»¤...")
#     filtered_dialogues = []
#     all_sentences = []
#
#     for q, a in dialogues:
#         q_tokens = simple_tokenize(q)
#         a_tokens = simple_tokenize(a)
#
#         # è¿‡æ»¤ï¼šé•¿åº¦åˆç† + éç©º
#         if (1 <= len(q_tokens) <= max_len and
#                 1 <= len(a_tokens) <= max_len):
#             filtered_dialogues.append((q_tokens, a_tokens))
#             all_sentences.extend([q_tokens, a_tokens])
#
#             if len(filtered_dialogues) >= max_samples:
#                 break
#
#     print(f"âœ… è¿‡æ»¤åå¯¹è¯æ•°é‡: {len(filtered_dialogues)}")
#
#     # æ„å»ºè¯è¡¨
#     print("ğŸ“š æ„å»ºè¯è¡¨...")
#     vocab = Vocabulary(freq_threshold=2)
#     vocab.build_vocabulary(all_sentences)
#     print(f"ğŸ“– è¯è¡¨å¤§å°: {len(vocab)}")
#
#     # æ•°æ®é›†åˆ’åˆ†
#     train_size = int(0.9 * len(filtered_dialogues))
#     train_pairs = filtered_dialogues[:train_size]
#     val_pairs = filtered_dialogues[train_size:]
#
#     print(f"ğŸ¯ è®­ç»ƒé›†: {len(train_pairs)} | éªŒè¯é›†: {len(val_pairs)}")
#
#     return train_pairs, val_pairs, vocab

def load_cornell_dialogues(data_dir:Path = Path('./data'), max_samples=5000, max_len=15):

    # æ•°æ®ä¸‹è½½
    corpus_dir = download_cornell_data(data_dir)

    # æ–‡ä»¶è·¯å¾„
    lines_file = corpus_dir / Path('movie_lines.txt')
    conv_file = corpus_dir / Path('movie_conversations.txt')
    assert(lines_file.exists() and conv_file.exists())

    # è§£ælinesæ–‡ä»¶ -> id2lines
    id2lines = dict()
    with open(lines_file, 'r', encoding='iso-8859-1') as f:
        for line in f:
            parts = line.strip().split(' +++$+++ ')
            if len(parts)==5:
                line_id = parts[0]
                text = parts[-1]
                id2lines[line_id]=text

    # è§£æconvæ–‡ä»¶ -> dialogues
    dialogues = []
    with open(conv_file, 'r', encoding='iso-8859-1') as f:
        for line in f:
            parts = line.strip().split(' +++$+++ ')
            if len(parts)==4:
                line_ids = eval(parts[-1])
                for i in range(len(line_ids)-1):
                    q_id, a_id = line_ids[i], line_ids[i+1]
                    if a_id in id2lines and q_id in id2lines:
                        dialogues.append((id2lines[a_id], id2lines[q_id]))

    print(f"åŸå§‹å¯¹è¯æ•°ï¼š{len(dialogues)}")

    # æ ¹æ®max_len max_samplesè¿‡æ»¤
    filtered_pairs = []
    all_sentences = []
    for q, a in dialogues:
        q_tokens = simple_tokenize(a)
        a_tokens = simple_tokenize(a)
        if (1<=len(a_tokens)<=max_len
                and 1<=len(q_tokens)<=max_len):
            filtered_pairs.append((q_tokens, a_tokens))
            all_sentences.extend([q_tokens, a_tokens])
            if len(filtered_pairs)>=max_samples:
                break

    # å»ºè¯å…¸
    vocab = Vocabulary(freq_threshold=2)
    vocab.build_vocabulary(all_sentences)
    print(f"è¯å…¸å¤§å°ï¼š{len(vocab)}")

    # åˆ’åˆ†æ•°æ®é›†
    train_size = int(0.9*len(filtered_pairs))
    train_pairs = filtered_pairs[:train_size]
    val_pairs = filtered_pairs[train_size:]

    return train_pairs, val_pairs, vocab







def create_dataloaders(train_pairs, val_pairs, vocab, batch_size=32):
    """åˆ›å»ºDataLoader"""
    train_dataset = DialogueDataset(train_pairs, vocab)
    val_dataset = DialogueDataset(val_pairs, vocab)

    # `lambda b: collate_fn(b, pad_idx=vocab.pad_token)` åœ¨å®šä¹‰ç¬é—´å°±æŠŠ `vocab.pad_token` æ•è·è¿›é—­åŒ…ï¼Œæ­¤å `pad_idx` å›ºå®šä¸å˜ï¼›
    # åªæœ‰ DataLoader æ¯æ¬¡å–‚è¿›æ¥çš„ `b` ä¼šéš batch å˜åŒ–ã€‚
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=lambda b: collate_fn(b, pad_idx=vocab.stoi[vocab.pad_token])
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=lambda b: collate_fn(b, pad_idx=vocab.stoi[vocab.pad_token])
    )

    return train_loader, val_loader


# ====================================
# ç¤ºä¾‹ï¼šæŸ¥çœ‹æ•°æ®
# ====================================
if __name__ == "__main__":
    # åŠ è½½æ•°æ®ï¼ˆåªå–5000å¯¹ï¼‰
    train_pairs, val_pairs, vocab = load_cornell_dialogues(
        max_samples=5000,
        max_len=15
    )

    # æŸ¥çœ‹ç¤ºä¾‹
    print("\n" + "=" * 50)
    print("ğŸ“ å¯¹è¯ç¤ºä¾‹:")
    print("=" * 50)
    for i in range(5):
        q, a = train_pairs[i]
        print(f"\nQ: {' '.join(q)}")
        print(f"A: {' '.join(a)}")

    # åˆ›å»ºDataLoader
    train_loader, val_loader = create_dataloaders(
        train_pairs, val_pairs, vocab, batch_size=4
    )

    # æŸ¥çœ‹ä¸€ä¸ªbatch
    src, tgt = next(iter(train_loader))
    print("\n" + "=" * 50)
    print("ğŸ“¦ Batchç¤ºä¾‹:")
    print("=" * 50)
    print(f"Source shape: {src.shape}")
    print(f"Target shape: {tgt.shape}")
    print(f"\nç¬¬ä¸€ä¸ªæ ·æœ¬:")
    print(f"Source IDs: {src[0].tolist()}")
    print(f"Target IDs: {tgt[0].tolist()}")
