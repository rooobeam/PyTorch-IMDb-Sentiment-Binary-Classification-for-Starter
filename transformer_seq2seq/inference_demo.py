# ====================================
# æ–‡ä»¶: inference_demo.py
# æ¨ç†æ¼”ç¤ºï¼ˆçœŸå®å¯¹è¯ç”Ÿæˆï¼‰
# ====================================

import torch
from config import TransformerConfig
from transformer import Transformer
from dataset import simple_tokenize


def greedy_decode(model, src, vocab, max_len=20):
    """
    è´ªå¿ƒè§£ç ï¼šæ¯æ¬¡é€‰æ¦‚ç‡æœ€å¤§çš„è¯
    src: [1, src_len] - å•ä¸ªå¥å­
    """
    model.eval()
    device = next(model.parameters()).device
    src = src.to(device)

    bos_id = vocab.stoi["<BOS>"]
    eos_id = vocab.stoi["<EOS>"]

    # Encoderä¸€æ¬¡æ€§ç¼–ç 
    enc_output = model.encode(src)
    src_mask = (src == model.pad_id).unsqueeze(1).unsqueeze(2)

    # Decoderè‡ªå›å½’ç”Ÿæˆ
    tgt = torch.tensor([[bos_id]], device=device)  # [1, 1]

    for _ in range(max_len):
        # é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
        logits = model.decode_step(tgt, enc_output, src_mask)  # [1, vocab_size]
        next_token = logits.argmax(dim=-1, keepdim=True)  # [1, 1]

        # æ‹¼æ¥åˆ°åºåˆ—
        tgt = torch.cat([tgt, next_token], dim=1)

        # é‡åˆ°ç»“æŸç¬¦åˆ™åœæ­¢
        if next_token.item() == eos_id:
            break

    return tgt.squeeze(0)  # [tgt_len]


def chat_demo():
    """äº¤äº’å¼å¯¹è¯æ¼”ç¤º"""

    # åŠ è½½æ¨¡å‹å’Œè¯è¡¨
    print("ğŸ¤– åŠ è½½æ¨¡å‹...")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    checkpoint = torch.load('transformer_cornell_full.pth', map_location=device, weights_only = False)
    vocab = checkpoint['vocab']
    config = checkpoint['config']


    print(f"device: {device}")
    model = Transformer(config).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.pad_id = config.PAD_ID
    model.eval()

    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\n")
    print("=" * 60)
    print("ğŸ’¬ å¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    print("=" * 60)
    print([name for name, _ in model.named_children()])
    print(f"æ¨¡å‹è¾“å‡ºå±‚ç»´åº¦: {model.output_proj.out_features}")
    print(f"çœŸå®è¯è¡¨å¤§å°: {len(vocab)}")
    while True:
        # ç”¨æˆ·è¾“å…¥
        user_input = input("\nä½ : ").strip()
        if user_input.lower() in ['quit', 'exit', 'q']:
            print("ğŸ‘‹ å†è§ï¼")
            break

        # åˆ†è¯
        tokens = simple_tokenize(user_input)
        if not tokens:
            print("Bot: ...")
            continue

        # è½¬ä¸ºID
        src_ids = [vocab.stoi["<BOS>"]] + vocab.numericalize(tokens) + [vocab.stoi["<EOS>"]]
        src = torch.tensor([src_ids])  # [1, src_len]

        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            output_ids = greedy_decode(model, src, vocab, max_len=20)

        # è§£ç ä¸ºæ–‡æœ¬
        output_tokens = [vocab.itos[idx.item()] for idx in output_ids]
        output_tokens = [t for t in output_tokens if t not in ["<BOS>", "<EOS>", "<PAD>"]]

        response = " ".join(output_tokens)
        print(f"Bot: {response}")


if __name__ == "__main__":
    chat_demo()
